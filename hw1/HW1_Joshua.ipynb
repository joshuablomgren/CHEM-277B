{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4121424",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from pylab import *\n",
    "import numpy.linalg as LA\n",
    "import numpy as np\n",
    "\n",
    "def timeit(f):\n",
    "\n",
    "    def timed(*args, **kw):\n",
    "\n",
    "        ts = time.time()\n",
    "        result = f(*args, **kw)\n",
    "        te = time.time()\n",
    "\n",
    "        print('func:%r took: %2.4f sec' % (f.__name__,  te-ts))\n",
    "        return result\n",
    "\n",
    "    return timed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e44eb2",
   "metadata": {},
   "source": [
    "# CHEM 277B: Homework 1 - Local Optimization Methods #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c86e2dae",
   "metadata": {},
   "source": [
    "## 1. Bisection vs. Golden Section ##\n",
    "\n",
    "<img src=\"Pictures/Bisection%20Graph.png\" width=\"300\" height=\"300\">\n",
    "\n",
    "In class we used the simple bisection method to take the first step in isolating at least one minimum for the function shown. This first step in placement of d reduced the original interval [a,b,c] =1.0 to [a,b,d] = 0.75. But in general, the average size interval <L> after Step 1 is determined by the equal probability of placing point d in either sub-interval, such that <L1>=P(left-interval) x 1‚ÅÑ2 + P(right-interval) x 3‚ÅÑ4 = 0.625 (since you can‚Äôt a priori know the best half)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834f0f82",
   "metadata": {},
   "source": [
    "#### (a) For step 2, place point e at the bisector of larger interval [a,b]. Why is this better than [b,d]? #####"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78913d5",
   "metadata": {},
   "source": [
    "<img src=\"Pictures/Bisection_e.png\" width=\"300\" height=\"300\">\n",
    "\n",
    "Placing point *e* at the bisector of larger interval $[a,b]$ is better than placing it at the bisector of $[b,d]$ because there is a greater chance of further reducing the interval. If point *e* were placed in $[b,d]$, while it could reduce the interval to $[b,d] = 0.25$ if $f(e) < f(b)$, if $f(e) > f(b)$, the interval would only be reduced to $[a,e] = 0.625$. On the other hand, point *e* placed at the bisector of $[b,d]$ would reduce the interval to $[a,b]$ or $[e,d]$, both of distance 0.5. This provides a higher chance of consistently reducing the search space. Placing points at the bisector of the larger interval could also prevent being trapped into a local minimum, as seen when point *e* is placed in $[b,d]$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2992596",
   "metadata": {},
   "source": [
    "#### (b) What is the new interval and how much is the search space reduced? ####"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbdf6575",
   "metadata": {},
   "source": [
    "The new interval according to the figure in problem 1a is $[a,e,b] = 0.5$, so the search spaced was reduced by $\\frac{1}{3}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e57c08",
   "metadata": {},
   "source": [
    "#### (c) For step 3, reduce the size of the interval from step 2 by placing point f at the bisection of your choice. ####"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d0aa6a",
   "metadata": {},
   "source": [
    "<img src=\"Pictures/1c.jpg\" width=\"300\" height=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a95a7ccc",
   "metadata": {},
   "source": [
    "Placing point *f* at the bisector of interval $[a,e]$ reduces the interval to $[f,e,b] = 0.375$, search spaced was reduced by $\\frac{1}{4}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79324aea",
   "metadata": {},
   "source": [
    "#### (d) fill in tree for all possible size intervals for steps 2 and 3. Write your answers in ratios to the interval size of the previous step. #### "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8eb37e",
   "metadata": {},
   "source": [
    "<img src=\"Pictures/1d.jpg\" width=\"500\" height=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eca5f4c",
   "metadata": {},
   "source": [
    " step 1: $\\frac{1}{2}$ and $\\frac{3}{4}$\n",
    " \n",
    " step 2: [$\\frac{1}{2}$ : $\\frac{3}{4}$ , $\\frac{1}{2}$] and [$\\frac{3}{4}$: $\\frac{2}{3}$, $\\frac{2}{3}$]\n",
    " \n",
    " step 3: [$\\frac{3}{4}$: $\\frac{2}{3}$, $\\frac{2}{3}$],  [$\\frac{1}{2}$: $\\frac{1}{2}$, $\\frac{3}{4}$], [$\\frac{2}{3}$: $\\frac{3}{4}$, $\\frac{1}{2}$], [$\\frac{2}{3}$: $\\frac{3}{4}$, $\\frac{1}{2}$]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f495d6d",
   "metadata": {},
   "source": [
    "#### (e) What is average size of interval at steps 2 and 3? ####"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9293afec",
   "metadata": {},
   "source": [
    "step 2: $P(0.25) \\times \\frac{3}{4} \\times \\frac{1}{2}+ P(0.25) \\times \\frac{1}{2} \\times \\frac{1}{2} + P(0.25) \\times \\frac{2}{3} \\times \\frac{3}{4} + P(0.25) \\times \\frac{2}{3} \\times \\frac{3}{4} = 0.406$\n",
    "\n",
    "step 3: $P(0.125) \\times \\frac{2}{3} \\times \\frac{3}{4} \\times \\frac{1}{2} + P(0.125) \\times \\frac{2}{3} \\times \\frac{3}{4} \\times \\frac{1}{2} + P(0.125) \\times \\frac{1}{2} \\times \\frac{1}{2} \\times \\frac{1}{2} + P(0.125) \\times \\frac{3}{4} \\times \\frac{1}{2} \\times \\frac{1}{2} + P(0.125) \\times \\frac{3}{4} \\times \\frac{2}{3} \\times \\frac{3}{4}+ P(0.125) \\times \\frac{1}{2} \\times \\frac{2}{3} \\times \\frac{3}{4} + P(0.125) \\times \\frac{3}{4} \\times \\frac{2}{3} \\times \\frac{3}{4} + P(0.125) \\times \\frac{1}{2} \\times \\frac{2}{3} \\times \\frac{3}{4} = 0.258$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17fc8253",
   "metadata": {},
   "source": [
    "#### (f) How much does Golden Section improve over Bisection at each step? Please use a chart of steps v.s. different methods to show their difference. ####"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c526ada",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "Golden Section: 0.618, 0.382, 0.236\n",
    "Bisection: 0.625, 0.646, 0.635"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b3672f",
   "metadata": {},
   "source": [
    "| Step | Bisection | Golden Section| \n",
    "|------|-----------|---------------|\n",
    "|1  | 0.625 | 0.618 | \n",
    "|2 | 0.406 | 0.382 | \n",
    "| 3 | 0.258 | 0.236 | "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e91335",
   "metadata": {},
   "source": [
    "## 2. Local optimization using 1st and quasi-2nd order methods. ##\n",
    "\n",
    "You will solve the following optimization problem using a python code you develop for the steepest descents method! For the function\n",
    "ùëì(ùë•, ùë¶) = ùë•4 ‚àí ùë•2 + ùë¶2 + 2ùë•ùë¶ ‚àí 2\n",
    "there are three stationary points found over the range x = [- 2,2] and y = [-2,2]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b027f0",
   "metadata": {},
   "source": [
    "#### (a) Starting from point (1.5,1.5), and with stepsize=0.1, determine new (ùë•, ùë¶) position using one step of the steepest descent algorithm (check against the debugging output). Is it a good optimization step? Depending on this outcome, how will you change the stepsize in the next step? #### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f59e8a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(position):\n",
    "    \n",
    "    x, y = position[0], position[1]\n",
    "    \n",
    "    # return the evaluation of the function \n",
    "    return x**4 - x**2 + y**2 + 2*x*y - 2\n",
    "\n",
    "def f_prime(position):\n",
    "    \n",
    "    x, y = position[0], position[1]\n",
    "    \n",
    "    # return the partial derivates of the function\n",
    "    return np.array([4*x**3 - 2*x + 2*y, 2*y + 2*x])\n",
    "\n",
    "def one_step_steepest_descent(func,first_derivative,starting_point,stepsize):\n",
    "    # determine new (x,y) position using one step of steepest descent \n",
    "    \n",
    "    # position after one step \n",
    "    new_position = starting_point - (stepsize*first_derivative(starting_point))\n",
    "    \n",
    "    print(f\"new (x,y) position: {new_position}\")\n",
    "    print(f\"f(x,y) for starting point: {f(starting_point)}\")\n",
    "    print(f\"f(x,y) for new position: {f(new_position)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2552fa36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new (x,y) position: [0.15 0.9 ]\n",
      "f(x,y) for starting point: 7.5625\n",
      "f(x,y) for new position: -0.9419937500000004\n"
     ]
    }
   ],
   "source": [
    "one_step_steepest_descent(f, f_prime, np.array([1.5, 1.5]), 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454de8ba",
   "metadata": {},
   "source": [
    "This is a good step because $f(x_1, y_1) < f(x_0, y_0)$ meaning that we are moving along a negative gradient and thus closer to the minimum. The stepsize will be multiplied by 1.2 since it was a good step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d98b3f",
   "metadata": {},
   "source": [
    "#### (b) Implement the steepest decent using the provided template. Continue executing steepest descents. How many steps does it take to converge to the local minimum to tolerance = 1 x 10-5 of the gradient (check against the debugging output and compare code timings)?\n",
    "Note: You don‚Äôt need to use line search, just take one step in the search direction, and use the following stepsize update: ùúÜ = {1.2 ùúÜ ùëìùëúùëü ùëé ùëîùëúùëúùëë ùë†ùë°ùëíùëù, 0.5ùúÜ ùëìùëúùëü ùëé ùëèùëéùëë ùë†ùë°ùëíùëù}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7987450",
   "metadata": {},
   "outputs": [],
   "source": [
    "@timeit\n",
    "def steepest_descent(func,first_derivative,starting_point,stepsize,tol):\n",
    "    # evaluate the gradient at starting point\n",
    "    \n",
    "    count = 0\n",
    "    visited = [starting_point]\n",
    "    deriv = first_derivative(starting_point)\n",
    "\n",
    "    while LA.norm(deriv) > tol and count < 1e6:\n",
    "        # calculate new point position\n",
    "        new_position = visited[-1] - (stepsize*first_derivative(visited[-1]))\n",
    "        deriv = first_derivative(new_position)\n",
    "                                         \n",
    "        if func(new_position) < func(visited[-1]):\n",
    "            # the step makes function evaluation lower - it is a good step. what do you do?\n",
    "            stepsize *= 1.2 \n",
    "            \n",
    "        else:\n",
    "            # the step makes function evaluation higher - it is a bad step. what do you do?\n",
    "            stepsize *= 0.5 \n",
    "        \n",
    "        visited.append(new_position)\n",
    "        count += 1\n",
    "                                         \n",
    "    # return the results\n",
    "    return {\"minimum\":new_position,\"evaluation\":func(new_position), \n",
    "            \"steps\":count, \"path\":np.asarray(visited)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8df745c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "func:'steepest_descent' took: 0.0025 sec\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'minimum': array([-0.99999892,  0.99999943]),\n",
       " 'evaluation': -2.999999999995068,\n",
       " 'steps': 45,\n",
       " 'path': array([[ 1.5       ,  1.5       ],\n",
       "        [ 0.15      ,  0.9       ],\n",
       "        [-0.03162   ,  0.648     ],\n",
       "        [-0.22733235,  0.47048256],\n",
       "        [-0.4603766 ,  0.38644985],\n",
       "        [-0.73063964,  0.41710875],\n",
       "        [-0.91361447,  0.57314178],\n",
       "        [-0.89067253,  0.77647099],\n",
       "        [-1.072703  ,  0.85831194],\n",
       "        [-0.88004038,  0.93513214],\n",
       "        [-1.07440969,  0.91144369],\n",
       "        [-0.8191814 ,  0.99553056],\n",
       "        [-1.00371455,  0.95003442],\n",
       "        [-0.98247032,  0.96665308],\n",
       "        [-1.00196259,  0.97252925],\n",
       "        [-0.98533102,  0.98565078],\n",
       "        [-1.0162044 ,  0.98547972],\n",
       "        [-0.99022477,  0.99369805],\n",
       "        [-1.00370679,  0.9925832 ],\n",
       "        [-0.9936794 ,  0.99686773],\n",
       "        [-1.00672832,  0.99539405],\n",
       "        [-0.99782619,  0.99801346],\n",
       "        [-1.00028169,  0.99796153],\n",
       "        [-0.99913443,  0.99873366],\n",
       "        [-1.00035525,  0.9988937 ],\n",
       "        [-0.99897351,  0.99959411],\n",
       "        [-1.00010453,  0.99944541],\n",
       "        [-0.99979477,  0.99963492],\n",
       "        [-1.00002278,  0.99969008],\n",
       "        [-0.9998473 ,  0.99982783],\n",
       "        [-1.00014104,  0.9998375 ],\n",
       "        [-0.99992545,  0.99991291],\n",
       "        [-1.0000106 ,  0.99991665],\n",
       "        [-0.99996182,  0.99995026],\n",
       "        [-1.00002241,  0.99995522],\n",
       "        [-0.99998875,  0.99996964],\n",
       "        [-0.99999542,  0.99997456],\n",
       "        [-0.99999464,  0.99998101],\n",
       "        [-0.99999754,  0.99998606],\n",
       "        [-0.99999681,  0.99999117],\n",
       "        [-1.00000061,  0.99999418],\n",
       "        [-0.99999493,  0.9999983 ],\n",
       "        [-1.00000251,  0.99999722],\n",
       "        [-0.99999661,  0.99999926],\n",
       "        [-1.00000408,  0.99999804],\n",
       "        [-0.99999892,  0.99999943]])}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "steepest_descent(f, f_prime, np.array([1.5, 1.5]), 0.1, 1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b180496a",
   "metadata": {},
   "source": [
    "It took 45 steps to converge to the local minimum (-1.0, 1.0)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280acf6d",
   "metadata": {},
   "source": [
    "#### (c) Compare your steepest descent code against conjugate gradients (CG), and BFGS to determine the local minimum starting from (1.5,1.5). In terms of number of steps, are conjugate gradients and/or BFGS more efficient than steepest descents? ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a593e3c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: -3.000000\n",
      "         Iterations: 9\n",
      "         Function evaluations: 78\n",
      "         Gradient evaluations: 26\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "     fun: -2.99999999999959\n",
       "     jac: array([ 2.08616257e-07, -1.10268593e-06])\n",
       " message: 'Optimization terminated successfully.'\n",
       "    nfev: 78\n",
       "     nit: 9\n",
       "    njev: 26\n",
       "  status: 0\n",
       " success: True\n",
       "       x: array([-0.99999984,  0.99999929])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "minimize(f, np.array([1.5, 1.5]), method='CG', options={'disp': True, 'gtol': 1e-5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5aa49f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: -3.000000\n",
      "         Iterations: 7\n",
      "         Function evaluations: 24\n",
      "         Gradient evaluations: 8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "      fun: -2.9999999999998255\n",
       " hess_inv: array([[ 0.12457729, -0.12457659],\n",
       "       [-0.12457659,  0.62569812]])\n",
       "      jac: array([-1.63912773e-06, -2.98023224e-08])\n",
       "  message: 'Optimization terminated successfully.'\n",
       "     nfev: 24\n",
       "      nit: 7\n",
       "     njev: 8\n",
       "   status: 0\n",
       "  success: True\n",
       "        x: array([ 0.99999979, -0.9999998 ])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minimize(f, np.array([1.5, 1.5]), method='BFGS', options={'disp': True, 'gtol': 1e-5})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f117ec",
   "metadata": {},
   "source": [
    "Conjugate gradients took 9 steps while BFGS took 7 steps. The steepest descent code took 45 steps, so CG and BFGS are much more efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4566db",
   "metadata": {},
   "source": [
    "## 3. Local optimization and machine learning using Stochastic Gradient Descent (SGD). ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d7f88d",
   "metadata": {},
   "source": [
    "The Rosenbrock Banana Function looks innocuous enough\n",
    "$ùëì(ùë•,ùë¶)=(1‚àíùë•)^2 +10(ùë¶‚àíùë•^2)^2$ with only one (global) minimum at $(ùë•, ùë¶) = (1.0,1.0)$!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff6a4f9",
   "metadata": {},
   "source": [
    "#### (a) Starting at ùë• = ‚àí0.5 and ùë¶ = 1.5, and using your code for steepest descents with stepsize =0.1, how many steps to converge to the minimum? Use a tolerance = 1 x 10-5 #### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15646fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rosenbrock(x):\n",
    "    return ((1 - x[0])**2 + 10*(x[1] - x[0]**2)**2)\n",
    "\n",
    "def derive_rosenbrock(x):\n",
    "    return np.array([-2*(1 - x[0]) - 40*x[0]*(x[1] - x[0]**2), 20*(x[1] - x[0]**2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4adaa829",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "func:'steepest_descent' took: 0.0020 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xf/ys07020s0656mhx3qrl86vfh0000gn/T/ipykernel_11894/1161317273.py:5: RuntimeWarning: overflow encountered in double_scalars\n",
      "  return np.array([-2*(1 - x[0]) - 40*x[0]*(x[1] - x[0]**2), 20*(x[1] - x[0]**2)])\n",
      "/var/folders/xf/ys07020s0656mhx3qrl86vfh0000gn/T/ipykernel_11894/1161317273.py:2: RuntimeWarning: overflow encountered in double_scalars\n",
      "  return ((1 - x[0])**2 + 10*(x[1] - x[0]**2)**2)\n",
      "/var/folders/xf/ys07020s0656mhx3qrl86vfh0000gn/T/ipykernel_11894/468941871.py:11: RuntimeWarning: invalid value encountered in subtract\n",
      "  new_position = visited[-1] - (stepsize*first_derivative(visited[-1]))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'minimum': array([nan, inf]),\n",
       " 'evaluation': nan,\n",
       " 'steps': 8,\n",
       " 'path': array([[-5.00000000e-001,  1.50000000e+000],\n",
       "        [-2.70000000e+000, -1.00000000e+000],\n",
       "        [ 4.24360000e+001,  7.29000000e+000],\n",
       "        [-7.60696243e+004,  9.04052048e+002],\n",
       "        [ 2.20091744e+014,  1.44664761e+009],\n",
       "        [-2.66533168e+042,  6.05504695e+027],\n",
       "        [ 2.36681219e+126,  4.43999561e+083],\n",
       "        [            -inf,  1.75056248e+251],\n",
       "        [             nan,              inf]])}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "steepest_descent(rosenbrock, derive_rosenbrock, np.array([-0.5, 1.5]), 0.1, 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9286bd32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "func:'steepest_descent' took: 0.0359 sec\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'minimum': array([0.99999095, 0.9999816 ]),\n",
       " 'evaluation': 8.280022388342527e-11,\n",
       " 'steps': 1165,\n",
       " 'path': array([[-0.5       ,  1.5       ],\n",
       "        [-0.72      ,  1.25      ],\n",
       "        [-0.93156096,  1.074416  ],\n",
       "        ...,\n",
       "        [ 0.99999058,  0.99998155],\n",
       "        [ 0.99999099,  0.99998145],\n",
       "        [ 0.99999095,  0.9999816 ]])}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "steepest_descent(rosenbrock, derive_rosenbrock, np.array([-0.5, 1.5]), 0.01, 1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7b3a36",
   "metadata": {},
   "source": [
    "Starting from x = -0.5 and y = 1.5 and using a stepsize of 0.1, the steepest descent function is unable to converge to the minimum, and overflow occurs really fast. Infinite values is reached after 8 steps. However, when I changed the stepsize to 0.01, it was able to find the (1.0, 1.0) minimum after 1165 steps. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea31fd77",
   "metadata": {},
   "source": [
    "#### (b) By adding a small amount of stochastic noise to the gradient at every step (In your code add a random vector that is the same norm as the gradient at that step), which is equivalent to a small batch derivative of any loss function in deep learning, implement your own stochastic gradient descent code by modifying on your steepest descent code, and run the SGD algorithm. (Check against debugging outputs.) ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2d633f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "@timeit\n",
    "def stochastic_gradient_descent(func,first_derivative,starting_point,\n",
    "                                stepsize,tol=1e-5,stochastic_injection=1):\n",
    "    '''stochastic_injection: controls the magnitude of stochasticity (multiplied with stochastic_deriv)\n",
    "        0 for no stochasticity, equivalent to SD. \n",
    "        Use 1 in this homework to run SGD\n",
    "    '''\n",
    "    # evaluate the gradient at starting point\n",
    "    \n",
    "    count=0\n",
    "    visited=[starting_point]\n",
    "    deriv = first_derivative(starting_point)\n",
    "    \n",
    "    while LA.norm(deriv) > tol and count < 1e5:\n",
    "        if stochastic_injection>0:\n",
    "            # formulate a stochastic_deriv that is the same norm as your gradient \n",
    "            # random vector (mean:0, sd:1, size:2)\n",
    "            stochastic_deriv = np.random.normal(0,1,len(starting_point))  \n",
    "            # calculate same norm as deriv \n",
    "            stochastic_deriv = stochastic_deriv/LA.norm(stochastic_deriv)*LA.norm(deriv) \n",
    "        else:\n",
    "            stochastic_deriv=np.zeros(len(starting_point))\n",
    "        \n",
    "        # calculate new point position\n",
    "        direction =- (deriv + stochastic_injection*stochastic_deriv)\n",
    "        new_position = visited[-1] + stepsize*direction\n",
    "        deriv = first_derivative(new_position)\n",
    "        \n",
    "        if func(new_position) < func(visited[-1]):\n",
    "            # the step makes function evaluation lower - it is a good step. what do you do?\n",
    "            stepsize *= 1.2\n",
    "            \n",
    "        else:\n",
    "            # the step makes function evaluation higher - it is a bad step. what do you do?\n",
    "            stepsize *= 0.5\n",
    "            \n",
    "        visited.append(new_position)\n",
    "        count+=1\n",
    "        \n",
    "    return {\"minimum\":new_position,\"evaluation\":func(new_position), \n",
    "            \"steps\":count, \"path\":np.asarray(visited)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bc6f189c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "func:'stochastic_gradient_descent' took: 0.0853 sec\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'minimum': array([0.99999152, 0.9999826 ]),\n",
       " 'evaluation': 7.384049766997677e-11,\n",
       " 'steps': 2023,\n",
       " 'path': array([[-0.5       ,  1.5       ],\n",
       "        [-0.59010287,  0.94336221],\n",
       "        [-0.58830064,  0.65936119],\n",
       "        ...,\n",
       "        [ 0.99999136,  0.99998255],\n",
       "        [ 0.99999143,  0.99998267],\n",
       "        [ 0.99999152,  0.9999826 ]])}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stochastic_gradient_descent(rosenbrock, derive_rosenbrock, np.array([-0.5, 1.5]), 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d29b78",
   "metadata": {},
   "source": [
    "#### (c) evaluate how much better or worse is the SGD convergence against the CG or BFGS method to find the global minimum, in terms of number of steps. Converge function/gradient to tolerance =1 √ó 10-5 ####"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9053734",
   "metadata": {},
   "source": [
    "CG:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d31db6be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.000000\n",
      "         Iterations: 20\n",
      "         Function evaluations: 132\n",
      "         Gradient evaluations: 44\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "     fun: 2.0711734599818967e-13\n",
       "     jac: array([ 4.95077415e-08, -2.45421994e-08])\n",
       " message: 'Optimization terminated successfully.'\n",
       "    nfev: 132\n",
       "     nit: 20\n",
       "    njev: 44\n",
       "  status: 0\n",
       " success: True\n",
       "       x: array([0.99999955, 0.99999908])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minimize(rosenbrock, np.array([-0.5, 1.5]), method='CG', \n",
    "         options={'disp': True, 'gtol': 1e-5})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003bd429",
   "metadata": {},
   "source": [
    "BFGS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bd24c2ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.000000\n",
      "         Iterations: 22\n",
      "         Function evaluations: 93\n",
      "         Gradient evaluations: 31\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "      fun: 1.6856836004019217e-13\n",
       " hess_inv: array([[0.50988602, 1.01962714],\n",
       "       [1.01962714, 2.08896666]])\n",
       "      jac: array([ 1.15312325e-07, -1.29424893e-08])\n",
       "  message: 'Optimization terminated successfully.'\n",
       "     nfev: 93\n",
       "      nit: 22\n",
       "     njev: 31\n",
       "   status: 0\n",
       "  success: True\n",
       "        x: array([0.99999959, 0.99999917])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minimize(rosenbrock, np.array([-0.5, 1.5]), method='BFGS', \n",
    "         options={'disp': True, 'gtol': 1e-5})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05431c1",
   "metadata": {},
   "source": [
    "SGD convergence performed a lot worse compared to the CG and BFGS methods, which only took 20 and 22 iterations respectively to find the global minumum. SGD took 1683 steps. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e6e6dc",
   "metadata": {},
   "source": [
    "#### (d) Can you draw a firm conclusion on the outcome with just one run of each method? If not, explain why ####"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e16d059",
   "metadata": {},
   "source": [
    "Yes, a firm conclusion can be drawn with just one run. Each of the three methods converges to approximately (1.0, 1.0) with one run, varying by the number of steps taken per run. Because they all converge to (1.0, 1.0) and according to the problem there is only one minimum in the Rosenbrock Banana function, we can assume the minimum is (1.0, 1.0). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f7665c",
   "metadata": {},
   "source": [
    "#### (e) Run all of the algorithms multiple times starting at different (x,y) positions to understand the average performance of each. Explain the relative performance of the non-stochastic and stochastic methods on the Rosenbrock Banana Function. #### "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ce9c46",
   "metadata": {},
   "source": [
    "SGD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e22c892c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_multiple_times(num_runs, func, first_derivative, stepsize, \n",
    "                       starting_point=0, tol=1e-5, stochastic_injection=1):\n",
    "    \"\"\"Run the algorithm multiple times and return the average number of steps taken to converge. \"\"\"\n",
    "\n",
    "    steps_SGD = []\n",
    "    steps_CG = []\n",
    "    steps_BFGS = []\n",
    "\n",
    "    print(f\"Running {num_runs} times\")\n",
    "\n",
    "    if type(starting_point) == int:\n",
    "        for i in range(num_runs):\n",
    "            starting_point = np.random.uniform(-3,3,2)\n",
    "            steps_SGD.append(stochastic_gradient_descent(func, \n",
    "                                                         first_derivative, \n",
    "                                                         starting_point, \n",
    "                                                         stepsize, tol, \n",
    "                                                         stochastic_injection)['steps'])\n",
    "            steps_CG.append(minimize(func, starting_point, \n",
    "                                     method='CG', \n",
    "                                     options={'disp': False, 'gtol': tol}).nit)\n",
    "            steps_BFGS.append(minimize(func, starting_point, \n",
    "                                       method='BFGS', \n",
    "                                       options={'disp': False, 'gtol': tol}).nit)\n",
    "\n",
    "            # in case of error, print the starting point\n",
    "            if steps_SGD[-1] < 30:\n",
    "                print(f\"SGD Overflow at {starting_point}\")\n",
    "    \n",
    "    else:\n",
    "        for i in range(num_runs):\n",
    "            steps_SGD.append(stochastic_gradient_descent(func, \n",
    "                                                         first_derivative, \n",
    "                                                         starting_point, \n",
    "                                                         stepsize, \n",
    "                                                         tol, \n",
    "                                                         stochastic_injection)['steps'])\n",
    "            steps_CG.append(minimize(func, starting_point, \n",
    "                                     method='CG', \n",
    "                                     options={'disp': False, 'gtol': tol}).nit)\n",
    "            steps_BFGS.append(minimize(func, \n",
    "                                       starting_point, method='BFGS', \n",
    "                                       options={'disp': False, 'gtol': tol}).nit)\n",
    "            \n",
    "    print (f\"Average number of steps for stochastic gradient descent: {np.mean(steps_SGD)}\")\n",
    "    print (f\"Average number of steps for conjugate gradient descent: {np.mean(steps_CG)}\")\n",
    "    print (f\"Average number of steps for BFGS: {np.mean(steps_BFGS)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6e0daeaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running 5 times\n",
      "func:'stochastic_gradient_descent' took: 0.1020 sec\n",
      "func:'stochastic_gradient_descent' took: 0.2174 sec\n",
      "func:'stochastic_gradient_descent' took: 0.0752 sec\n",
      "func:'stochastic_gradient_descent' took: 0.0804 sec\n",
      "func:'stochastic_gradient_descent' took: 0.0770 sec\n",
      "Average number of steps for stochastic gradient descent: 2751.6\n",
      "Average number of steps for conjugate gradient descent: 16.2\n",
      "Average number of steps for BFGS: 20.2\n"
     ]
    }
   ],
   "source": [
    "run_multiple_times(5, rosenbrock, derive_rosenbrock, 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b161be",
   "metadata": {},
   "source": [
    "The average number of steps for stochastic gradient descent is significantly higher than those of CG and BFGS. In addition, in the range of (-3,3), there are some starting positions that cause the SGD function to crash due to overflow of the position values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baee0f1f",
   "metadata": {},
   "source": [
    "## 4. Stochastic Gradient Descent with Momentum (SGDM) ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2534fdb8",
   "metadata": {},
   "source": [
    "The Rosenbrock Banana function with one minimum is not the best way to illustrate the power of the SGD or SGDM method. Hence we next investigate the Three-Hump Camel function:\n",
    "$$ùëì(ùë•, ùë¶) = 2ùë•^2 ‚àí 1.05ùë•^4 + ùë•^6‚ÅÑ6 + ùë•ùë¶ + ùë¶^2 $$\n",
    "$$ùë• ‚àà [‚àí2,2], ùë¶ ‚àà [‚àí2,2] $$\n",
    "which is a convex function with three minima. This defines our first ‚Äúmultiple minima‚Äù problem where there is a global solution as well as two less optimal solutions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa5fc88",
   "metadata": {},
   "source": [
    "#### (a) Utilize SGD to find the global minimum, and compare it to CG or BFGS as you did in (2e). Starting from [-1.5,-1.5], converge function and gradient to tolerance = 1 √ó 10-5 with stepsize =0.1. On average, did you get a better result in finding the global minimum with SGD in terms of fewer steps on average? ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ea0d76c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def three_hump_camel(x):\n",
    "    return 2*x[0]**2 - 1.05*x[0]**4 + x[0]**6/6 + x[0]*x[1] + x[1]**2\n",
    "\n",
    "def derive_three_hump_camel(x):\n",
    "    return np.array([4*x[0] - 4.2*x[0]**3 + x[0]**5 + x[1], x[0] + 2*x[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c5117723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "func:'stochastic_gradient_descent' took: 0.0032 sec\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'minimum': array([ 2.23473805e-06, -2.14392181e-06]),\n",
       " 'evaluation': 9.793405418418065e-12,\n",
       " 'steps': 44,\n",
       " 'path': array([[-1.50000000e+00, -1.50000000e+00],\n",
       "        [-1.08417542e+00, -7.24427140e-01],\n",
       "        [-1.14034094e+00, -1.50327565e-01],\n",
       "        [-9.24169333e-01, -9.10551337e-02],\n",
       "        [-4.58843570e-01,  1.66660707e-01],\n",
       "        [-4.42274477e-01,  9.66614342e-02],\n",
       "        [-2.78875832e-01, -1.32881109e-01],\n",
       "        [ 4.32439197e-01, -8.34224026e-02],\n",
       "        [ 2.06497774e-01,  7.61729246e-02],\n",
       "        [-1.47721838e-02, -1.42384892e-01],\n",
       "        [ 1.05530716e-01, -8.77611724e-02],\n",
       "        [ 2.80383806e-02,  1.68602341e-02],\n",
       "        [-1.79883376e-02,  4.15716593e-02],\n",
       "        [-2.64389950e-04,  4.32803303e-02],\n",
       "        [ 7.67938893e-03,  3.55718232e-02],\n",
       "        [-1.08618135e-02, -4.64399986e-03],\n",
       "        [-8.33474890e-03,  1.01089670e-02],\n",
       "        [ 8.30623060e-04,  1.44929358e-02],\n",
       "        [-5.09283779e-03,  5.05110371e-03],\n",
       "        [-1.02820567e-03,  6.98359326e-03],\n",
       "        [ 7.76935630e-04,  5.80658723e-03],\n",
       "        [-1.47178636e-03, -1.85663766e-03],\n",
       "        [ 2.05479567e-03, -3.08357725e-03],\n",
       "        [ 6.12949182e-04, -1.47542552e-03],\n",
       "        [ 6.93792604e-04, -5.84501406e-04],\n",
       "        [ 5.82809790e-04, -8.12921461e-04],\n",
       "        [ 6.74799366e-04, -4.78922547e-04],\n",
       "        [-1.63810593e-04, -1.15012708e-03],\n",
       "        [ 5.45307008e-04, -3.79216885e-04],\n",
       "        [ 2.38174055e-04,  3.47332377e-05],\n",
       "        [ 2.49695872e-04, -4.91451210e-05],\n",
       "        [-3.15689916e-04, -4.43086031e-05],\n",
       "        [-9.84473354e-05,  2.18655271e-04],\n",
       "        [-1.04368676e-04,  1.00879369e-04],\n",
       "        [-1.06042448e-04,  9.43522410e-05],\n",
       "        [ 3.11166985e-05,  1.43376773e-04],\n",
       "        [-5.33483129e-05,  8.32410007e-05],\n",
       "        [-5.97390525e-05,  6.29716818e-05],\n",
       "        [-6.19039924e-05,  4.85166787e-05],\n",
       "        [-1.61383347e-05, -4.26940095e-06],\n",
       "        [-7.01350923e-06, -1.48717313e-05],\n",
       "        [ 1.22684243e-05, -2.03389516e-05],\n",
       "        [ 4.85123397e-06, -9.95548290e-06],\n",
       "        [ 5.47716431e-06, -9.45116338e-06],\n",
       "        [ 2.23473805e-06, -2.14392181e-06]])}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stochastic_gradient_descent(three_hump_camel, \n",
    "                            derive_three_hump_camel, \n",
    "                            np.array([-1.5, -1.5]), 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fc41c68d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.298638\n",
      "         Iterations: 8\n",
      "         Function evaluations: 30\n",
      "         Gradient evaluations: 10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "      fun: 0.29863844223686065\n",
       " hess_inv: array([[ 0.08568628, -0.04289906],\n",
       "       [-0.04289906,  0.51091221]])\n",
       "      jac: array([ 1.34110451e-07, -1.49011612e-08])\n",
       "  message: 'Optimization terminated successfully.'\n",
       "     nfev: 30\n",
       "      nit: 8\n",
       "     njev: 10\n",
       "   status: 0\n",
       "  success: True\n",
       "        x: array([-1.74755234,  0.87377616])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minimize(three_hump_camel, np.array([-1.5, -1.5]), \n",
    "         method='BFGS', options={'disp': True, 'gtol': 1e-5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d039fe45",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.298638\n",
      "         Iterations: 7\n",
      "         Function evaluations: 63\n",
      "         Gradient evaluations: 21\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "     fun: 0.2986384422397371\n",
       "     jac: array([8.40425491e-06, 7.45058060e-07])\n",
       " message: 'Optimization terminated successfully.'\n",
       "    nfev: 63\n",
       "     nit: 7\n",
       "    njev: 21\n",
       "  status: 0\n",
       " success: True\n",
       "       x: array([-1.74755166,  0.87377619])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minimize(three_hump_camel, np.array([-1.5, -1.5]), method='CG', \n",
    "         options={'disp': True, 'gtol': 1e-5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "06274b39",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running 5 times\n",
      "func:'stochastic_gradient_descent' took: 0.0071 sec\n",
      "func:'stochastic_gradient_descent' took: 0.0039 sec\n",
      "func:'stochastic_gradient_descent' took: 0.0021 sec\n",
      "func:'stochastic_gradient_descent' took: 0.0019 sec\n",
      "func:'stochastic_gradient_descent' took: 0.0044 sec\n",
      "Average number of steps for stochastic gradient descent: 68.6\n",
      "Average number of steps for conjugate gradient descent: 7.0\n",
      "Average number of steps for BFGS: 8.0\n"
     ]
    }
   ],
   "source": [
    "run_multiple_times(5, three_hump_camel, derive_three_hump_camel, \n",
    "                   0.1, np.array([-1.5, -1.5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf44fbe",
   "metadata": {},
   "source": [
    "In terms of average steps to find the minimum, CG and BFGS were better. However, the three functions all do not converge to the global minimum (0,0) a lot of the time. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc4f112",
   "metadata": {},
   "source": [
    "#### (b) Implement the SGDM algorithm with momentum ùõæ = 0.9. Now use SGD with Momentum to find the global minimum. Again start from [-1.5,-1.5] with stepsize = 0.1 and converge function  and gradient to tolerance = $1√ó10^{-5}$. On average,did you ge a better result using SGDM compared to SGD, CG or BFGS in finding the global minimum in terms of fewer steps? #### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e9dfdadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "@timeit\n",
    "def SGDM(func,first_derivative,starting_point,stepsize,momentum=0.9,\n",
    "         tol=1e-5,stochastic_injection=1):\n",
    "    # evaluate the gradient at starting point\n",
    "    \n",
    "    count = 0\n",
    "    visited = [starting_point]\n",
    "    deriv = first_derivative(starting_point)\n",
    "    previous_direction = np.zeros(len(starting_point))\n",
    "\n",
    "    while LA.norm(deriv) > tol and count < 1e5:\n",
    "        if stochastic_injection>0:\n",
    "            # formulate a stochastic_deriv that is the same norm as deriv\n",
    "            stochastic_deriv = np.random.normal(0,1,2) # random vector with mean 0 and std 1 \n",
    "            stochastic_deriv = stochastic_deriv/LA.norm(stochastic_deriv)*LA.norm(deriv) \n",
    "        else: \n",
    "            stochastic_deriv = np.zeros(len(starting_point))\n",
    "        \n",
    "        direction =- (deriv + stochastic_injection*stochastic_deriv)\n",
    "        direction = momentum*previous_direction + direction\n",
    "        new_position = visited[-1] + stepsize*direction\n",
    "        previous_direction = direction\n",
    "        deriv = first_derivative(new_position)\n",
    "\n",
    "        if func(new_position) < func(visited[-1]):\n",
    "            stepsize *= 1.2\n",
    "        else:\n",
    "            stepsize *= 0.5\n",
    "\n",
    "            if stepsize < 1e-5:\n",
    "                previous_direction = np.zeros(len(starting_point))\n",
    "\n",
    "        visited.append(new_position)\n",
    "        count += 1\n",
    "\n",
    "    return {\"minimum\":new_position,\"evaluation\":func(new_position), \"steps\":count, \"path\":np.asarray(visited)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d7de4eb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "func:'SGDM' took: 0.0296 sec\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'minimum': array([-9.81736841e-07,  2.06759961e-06]),\n",
       " 'evaluation': 4.172743886762954e-12,\n",
       " 'steps': 438,\n",
       " 'path': array([[-1.50000000e+00, -1.50000000e+00],\n",
       "        [-1.02558206e+00, -7.95830937e-01],\n",
       "        [-1.21032131e-01, -1.48924517e-02],\n",
       "        [ 9.91888393e-01,  8.85307298e-01],\n",
       "        [ 1.39329757e+00,  1.32434774e+00],\n",
       "        [ 1.41674285e+00,  1.30301602e+00],\n",
       "        [ 1.57086765e+00,  1.17993613e+00],\n",
       "        [ 1.81992517e+00,  6.75040380e-01],\n",
       "        [ 1.71518685e+00, -2.09723771e-01],\n",
       "        [ 1.67878562e+00, -1.26398891e+00],\n",
       "        [ 1.74996931e+00, -2.44931496e+00],\n",
       "        [ 1.87085348e+00, -2.68580215e+00],\n",
       "        [ 1.89612227e+00, -2.63913817e+00],\n",
       "        [ 1.81747401e+00, -2.53937005e+00],\n",
       "        [ 1.82496475e+00, -2.40652709e+00],\n",
       "        [ 1.94992709e+00, -2.09038012e+00],\n",
       "        [ 2.11721214e+00, -1.60792462e+00],\n",
       "        [ 1.75015471e+00, -1.33351011e+00],\n",
       "        [ 1.38135730e+00, -9.88182278e-01],\n",
       "        [ 1.21673372e+00, -8.35580262e-01],\n",
       "        [ 1.14822289e+00, -7.70548556e-01],\n",
       "        [ 1.11706012e+00, -7.39202154e-01],\n",
       "        [ 1.10280953e+00, -7.24663658e-01],\n",
       "        [ 1.08710296e+00, -7.08148109e-01],\n",
       "        [ 1.06974936e+00, -6.89739152e-01],\n",
       "        [ 1.05110387e+00, -6.67952190e-01],\n",
       "        [ 1.03176395e+00, -6.42524647e-01],\n",
       "        [ 1.01014363e+00, -6.14927432e-01],\n",
       "        [ 9.86029165e-01, -5.82601635e-01],\n",
       "        [ 9.59612909e-01, -5.48105881e-01],\n",
       "        [ 9.26218502e-01, -5.07380678e-01],\n",
       "        [ 8.80551341e-01, -4.63997926e-01],\n",
       "        [ 8.21801804e-01, -4.24233190e-01],\n",
       "        [ 7.36119731e-01, -3.75696317e-01],\n",
       "        [ 6.36598272e-01, -3.09184832e-01],\n",
       "        [ 5.24881092e-01, -2.24249382e-01],\n",
       "        [ 3.72106705e-01, -1.05079158e-01],\n",
       "        [ 1.81695849e-01,  5.04908389e-02],\n",
       "        [-5.76072005e-02,  2.33496260e-01],\n",
       "        [-3.26848367e-01,  4.03625287e-01],\n",
       "        [-4.37573545e-01,  4.54051837e-01],\n",
       "        [-4.68373213e-01,  4.68787639e-01],\n",
       "        [-4.82335634e-01,  4.75186899e-01],\n",
       "        [-4.84577251e-01,  4.74783076e-01],\n",
       "        [-4.84309883e-01,  4.75383897e-01],\n",
       "        [-4.84157500e-01,  4.75715348e-01],\n",
       "        [-4.83843539e-01,  4.76054248e-01],\n",
       "        [-4.82756539e-01,  4.76121580e-01],\n",
       "        [-4.80882516e-01,  4.76396861e-01],\n",
       "        [-4.78534831e-01,  4.75904476e-01],\n",
       "        [-4.75422178e-01,  4.74384136e-01],\n",
       "        [-4.70797909e-01,  4.71731402e-01],\n",
       "        [-4.64297784e-01,  4.67654178e-01],\n",
       "        [-4.56968307e-01,  4.63650616e-01],\n",
       "        [-4.48546817e-01,  4.57529910e-01],\n",
       "        [-4.39535557e-01,  4.49677538e-01],\n",
       "        [-4.29186127e-01,  4.41947403e-01],\n",
       "        [-4.18234272e-01,  4.32435359e-01],\n",
       "        [-4.03790659e-01,  4.18145726e-01],\n",
       "        [-3.82470422e-01,  4.03221720e-01],\n",
       "        [-3.52818391e-01,  3.87698634e-01],\n",
       "        [-3.12589358e-01,  3.69965809e-01],\n",
       "        [-2.61320472e-01,  3.51658799e-01],\n",
       "        [-1.97888005e-01,  3.32198190e-01],\n",
       "        [-1.22021470e-01,  3.04205248e-01],\n",
       "        [-3.38399840e-02,  2.67227093e-01],\n",
       "        [ 5.95778128e-02,  2.15513622e-01],\n",
       "        [ 1.02858497e-01,  1.89392744e-01],\n",
       "        [ 1.18443156e-01,  1.75773240e-01],\n",
       "        [ 1.24100170e-01,  1.67894989e-01],\n",
       "        [ 1.26559982e-01,  1.64446997e-01],\n",
       "        [ 1.27405991e-01,  1.63020545e-01],\n",
       "        [ 1.27737160e-01,  1.61095919e-01],\n",
       "        [ 1.28160271e-01,  1.58709579e-01],\n",
       "        [ 1.28511215e-01,  1.55461384e-01],\n",
       "        [ 1.28772027e-01,  1.51163694e-01],\n",
       "        [ 1.27978817e-01,  1.46650630e-01],\n",
       "        [ 1.27131869e-01,  1.41761166e-01],\n",
       "        [ 1.25903986e-01,  1.35108292e-01],\n",
       "        [ 1.23195182e-01,  1.28392647e-01],\n",
       "        [ 1.17818330e-01,  1.20107678e-01],\n",
       "        [ 1.10554420e-01,  1.08830619e-01],\n",
       "        [ 1.00077878e-01,  9.70205964e-02],\n",
       "        [ 8.88543463e-02,  8.40928248e-02],\n",
       "        [ 7.31899287e-02,  6.97729489e-02],\n",
       "        [ 5.26684464e-02,  5.35738698e-02],\n",
       "        [ 3.06082094e-02,  3.58810980e-02],\n",
       "        [ 4.71531834e-03,  1.53357753e-02],\n",
       "        [-2.33080548e-02, -7.46801335e-03],\n",
       "        [-3.59306020e-02, -1.74593513e-02],\n",
       "        [-4.11017381e-02, -2.15136534e-02],\n",
       "        [-4.34449434e-02, -2.32249673e-02],\n",
       "        [-4.44511700e-02, -2.38539803e-02],\n",
       "        [-4.48307070e-02, -2.41645366e-02],\n",
       "        [-4.49481501e-02, -2.42899685e-02],\n",
       "        [-4.49746315e-02, -2.43440742e-02],\n",
       "        [-4.49753472e-02, -2.43706127e-02],\n",
       "        [-4.49759105e-02, -2.43803404e-02],\n",
       "        [-4.49760091e-02, -2.43791427e-02],\n",
       "        [-4.49761393e-02, -2.43777956e-02],\n",
       "        [-4.49714667e-02, -2.43749722e-02],\n",
       "        [-4.49627861e-02, -2.43677446e-02],\n",
       "        [-4.49498363e-02, -2.43547813e-02],\n",
       "        [-4.49342718e-02, -2.43426135e-02],\n",
       "        [-4.49152323e-02, -2.43318732e-02],\n",
       "        [-4.48839701e-02, -2.43138069e-02],\n",
       "        [-4.48429076e-02, -2.42986757e-02],\n",
       "        [-4.47858969e-02, -2.42706793e-02],\n",
       "        [-4.47223961e-02, -2.42433171e-02],\n",
       "        [-4.46430544e-02, -2.42212886e-02],\n",
       "        [-4.45517770e-02, -2.42039989e-02],\n",
       "        [-4.44474099e-02, -2.41924758e-02],\n",
       "        [-4.43057538e-02, -2.41912728e-02],\n",
       "        [-4.41298697e-02, -2.41520963e-02],\n",
       "        [-4.38892435e-02, -2.40728961e-02],\n",
       "        [-4.35760460e-02, -2.40046302e-02],\n",
       "        [-4.32397984e-02, -2.38978244e-02],\n",
       "        [-4.27980424e-02, -2.38052224e-02],\n",
       "        [-4.22438881e-02, -2.37397903e-02],\n",
       "        [-4.15090659e-02, -2.36737700e-02],\n",
       "        [-4.06894864e-02, -2.36351550e-02],\n",
       "        [-3.98054153e-02, -2.35044884e-02],\n",
       "        [-3.87525796e-02, -2.34341786e-02],\n",
       "        [-3.74619047e-02, -2.34405976e-02],\n",
       "        [-3.57427316e-02, -2.34020567e-02],\n",
       "        [-3.38529822e-02, -2.34080483e-02],\n",
       "        [-3.15225598e-02, -2.35240837e-02],\n",
       "        [-2.90184955e-02, -2.34341531e-02],\n",
       "        [-2.57702325e-02, -2.32742469e-02],\n",
       "        [-2.21908986e-02, -2.31873693e-02],\n",
       "        [-1.77198796e-02, -2.27363609e-02],\n",
       "        [-1.23682714e-02, -2.17591739e-02],\n",
       "        [-6.57204761e-03, -2.02532580e-02],\n",
       "        [-1.93179936e-04, -1.86997841e-02],\n",
       "        [ 6.71041103e-03, -1.65976429e-02],\n",
       "        [ 1.42620862e-02, -1.42451532e-02],\n",
       "        [ 1.76438132e-02, -1.32208998e-02],\n",
       "        [ 1.91191324e-02, -1.26620736e-02],\n",
       "        [ 1.97400994e-02, -1.24578324e-02],\n",
       "        [ 2.00123181e-02, -1.23825338e-02],\n",
       "        [ 2.01348121e-02, -1.23487105e-02],\n",
       "        [ 2.01783431e-02, -1.23385277e-02],\n",
       "        [ 2.01908501e-02, -1.23331677e-02],\n",
       "        [ 2.01964399e-02, -1.23302538e-02],\n",
       "        [ 2.01982123e-02, -1.23280043e-02],\n",
       "        [ 2.01980738e-02, -1.23282994e-02],\n",
       "        [ 2.01979078e-02, -1.23284471e-02],\n",
       "        [ 2.01976078e-02, -1.23289415e-02],\n",
       "        [ 2.01957524e-02, -1.23295445e-02],\n",
       "        [ 2.01929220e-02, -1.23310556e-02],\n",
       "        [ 2.01897689e-02, -1.23330728e-02],\n",
       "        [ 2.01856651e-02, -1.23339912e-02],\n",
       "        [ 2.01810151e-02, -1.23356899e-02],\n",
       "        [ 2.01759888e-02, -1.23375783e-02],\n",
       "        [ 2.01704385e-02, -1.23402201e-02],\n",
       "        [ 2.01598438e-02, -1.23408436e-02],\n",
       "        [ 2.01471444e-02, -1.23438995e-02],\n",
       "        [ 2.01291550e-02, -1.23429824e-02],\n",
       "        [ 2.01097072e-02, -1.23411420e-02],\n",
       "        [ 2.00860556e-02, -1.23339503e-02],\n",
       "        [ 2.00602397e-02, -1.23237622e-02],\n",
       "        [ 2.00308026e-02, -1.23073800e-02],\n",
       "        [ 1.99907602e-02, -1.22987133e-02],\n",
       "        [ 1.99241017e-02, -1.22896372e-02],\n",
       "        [ 1.98256647e-02, -1.22853827e-02],\n",
       "        [ 1.96859912e-02, -1.22777294e-02],\n",
       "        [ 1.94960065e-02, -1.22732291e-02],\n",
       "        [ 1.92861881e-02, -1.22807158e-02],\n",
       "        [ 1.90036091e-02, -1.22876064e-02],\n",
       "        [ 1.86577487e-02, -1.23243325e-02],\n",
       "        [ 1.82091064e-02, -1.23737047e-02],\n",
       "        [ 1.77072329e-02, -1.24579959e-02],\n",
       "        [ 1.70613194e-02, -1.25518931e-02],\n",
       "        [ 1.62456620e-02, -1.26292721e-02],\n",
       "        [ 1.52746664e-02, -1.27657211e-02],\n",
       "        [ 1.42276313e-02, -1.28953677e-02],\n",
       "        [ 1.30894991e-02, -1.29739229e-02],\n",
       "        [ 1.16819070e-02, -1.30457171e-02],\n",
       "        [ 1.00819769e-02, -1.31820377e-02],\n",
       "        [ 8.35071171e-03, -1.33344661e-02],\n",
       "        [ 6.33517125e-03, -1.35073901e-02],\n",
       "        [ 4.20427407e-03, -1.35542055e-02],\n",
       "        [ 1.81622115e-03, -1.33698609e-02],\n",
       "        [-8.30047660e-04, -1.31373059e-02],\n",
       "        [-2.05202615e-03, -1.30036833e-02],\n",
       "        [-2.51303588e-03, -1.28738731e-02],\n",
       "        [-2.72032259e-03, -1.28155935e-02],\n",
       "        [-2.81873979e-03, -1.27809201e-02],\n",
       "        [-2.86395025e-03, -1.27643897e-02],\n",
       "        [-2.87924921e-03, -1.27569354e-02],\n",
       "        [-2.88674286e-03, -1.27526227e-02],\n",
       "        [-2.88993116e-03, -1.27508059e-02],\n",
       "        [-2.89058974e-03, -1.27497712e-02],\n",
       "        [-2.89133802e-03, -1.27486191e-02],\n",
       "        [-2.89116954e-03, -1.27463811e-02],\n",
       "        [-2.89031985e-03, -1.27441566e-02],\n",
       "        [-2.88970136e-03, -1.27413101e-02],\n",
       "        [-2.88706114e-03, -1.27371117e-02],\n",
       "        [-2.88312792e-03, -1.27329183e-02],\n",
       "        [-2.87710476e-03, -1.27286809e-02],\n",
       "        [-2.86716803e-03, -1.27224902e-02],\n",
       "        [-2.85324387e-03, -1.27158428e-02],\n",
       "        [-2.83900701e-03, -1.27075809e-02],\n",
       "        [-2.82254150e-03, -1.26925750e-02],\n",
       "        [-2.79971229e-03, -1.26767361e-02],\n",
       "        [-2.76937832e-03, -1.26511764e-02],\n",
       "        [-2.73751950e-03, -1.26151649e-02],\n",
       "        [-2.69474452e-03, -1.25643527e-02],\n",
       "        [-2.63423748e-03, -1.25027180e-02],\n",
       "        [-2.57204378e-03, -1.24248039e-02],\n",
       "        [-2.50909921e-03, -1.23335464e-02],\n",
       "        [-2.42673599e-03, -1.22100600e-02],\n",
       "        [-2.31549148e-03, -1.20504049e-02],\n",
       "        [-2.20165284e-03, -1.18691687e-02],\n",
       "        [-2.08692979e-03, -1.16606881e-02],\n",
       "        [-1.97347707e-03, -1.14165840e-02],\n",
       "        [-1.80913246e-03, -1.11512752e-02],\n",
       "        [-1.60227422e-03, -1.08718424e-02],\n",
       "        [-1.32867572e-03, -1.04998970e-02],\n",
       "        [-9.56263454e-04, -1.00598555e-02],\n",
       "        [-5.78258591e-04, -9.53658700e-03],\n",
       "        [-9.14535541e-05, -8.88711692e-03],\n",
       "        [ 4.03407088e-04, -8.14835094e-03],\n",
       "        [ 1.00161576e-03, -7.34138743e-03],\n",
       "        [ 1.66120608e-03, -6.47163053e-03],\n",
       "        [ 2.36349548e-03, -5.41291102e-03],\n",
       "        [ 3.10340681e-03, -4.15601026e-03],\n",
       "        [ 3.81384807e-03, -2.82901902e-03],\n",
       "        [ 4.05508252e-03, -2.18786381e-03],\n",
       "        [ 4.12569665e-03, -1.87422000e-03],\n",
       "        [ 4.15275749e-03, -1.72295151e-03],\n",
       "        [ 4.15923329e-03, -1.64822532e-03],\n",
       "        [ 4.15787476e-03, -1.61127549e-03],\n",
       "        [ 4.15381235e-03, -1.59404209e-03],\n",
       "        [ 4.14690052e-03, -1.57770022e-03],\n",
       "        [ 4.13819427e-03, -1.55800529e-03],\n",
       "        [ 4.12383465e-03, -1.53944232e-03],\n",
       "        [ 4.10276730e-03, -1.52288614e-03],\n",
       "        [ 4.07537332e-03, -1.50976247e-03],\n",
       "        [ 4.03990997e-03, -1.49069749e-03],\n",
       "        [ 3.98900663e-03, -1.47053854e-03],\n",
       "        [ 3.92067929e-03, -1.44470594e-03],\n",
       "        [ 3.84153310e-03, -1.40932469e-03],\n",
       "        [ 3.75346385e-03, -1.36498034e-03],\n",
       "        [ 3.65677321e-03, -1.32405001e-03],\n",
       "        [ 3.52527513e-03, -1.28702091e-03],\n",
       "        [ 3.35723560e-03, -1.23488091e-03],\n",
       "        [ 3.17390840e-03, -1.18815664e-03],\n",
       "        [ 2.94132832e-03, -1.12260222e-03],\n",
       "        [ 2.68574749e-03, -1.03955599e-03],\n",
       "        [ 2.36095177e-03, -9.39713695e-04],\n",
       "        [ 1.98899730e-03, -8.06923216e-04],\n",
       "        [ 1.56669759e-03, -6.91482220e-04],\n",
       "        [ 1.10559947e-03, -5.82888461e-04],\n",
       "        [ 5.79002826e-04, -4.45483192e-04],\n",
       "        [ 8.90893555e-07, -3.07199930e-04],\n",
       "        [-6.26480581e-04, -1.53328919e-04],\n",
       "        [-9.09413889e-04, -7.98201973e-05],\n",
       "        [-1.02919722e-03, -3.69524703e-05],\n",
       "        [-1.07764080e-03, -1.25233302e-05],\n",
       "        [-1.09724331e-03, -3.19471068e-06],\n",
       "        [-1.10538426e-03,  2.32231804e-06],\n",
       "        [-1.10798025e-03,  5.16161624e-06],\n",
       "        [-1.10865241e-03,  6.33204710e-06],\n",
       "        [-1.10889769e-03,  6.77618090e-06],\n",
       "        [-1.10887731e-03,  7.02669784e-06],\n",
       "        [-1.10869458e-03,  7.28527242e-06],\n",
       "        [-1.10847354e-03,  7.65759845e-06],\n",
       "        [-1.10800127e-03,  8.12969394e-06],\n",
       "        [-1.10745199e-03,  8.57051351e-06],\n",
       "        [-1.10651360e-03,  9.10553972e-06],\n",
       "        [-1.10520639e-03,  9.92420702e-06],\n",
       "        [-1.10364799e-03,  1.11008515e-05],\n",
       "        [-1.10145920e-03,  1.22288981e-05],\n",
       "        [-1.09838257e-03,  1.35248544e-05],\n",
       "        [-1.09484934e-03,  1.54055752e-05],\n",
       "        [-1.09007747e-03,  1.78073516e-05],\n",
       "        [-1.08494063e-03,  2.05755807e-05],\n",
       "        [-1.07877300e-03,  2.30080334e-05],\n",
       "        [-1.07071376e-03,  2.51515826e-05],\n",
       "        [-1.06107258e-03,  2.66681197e-05],\n",
       "        [-1.05069411e-03,  2.85959020e-05],\n",
       "        [-1.03820644e-03,  2.95528351e-05],\n",
       "        [-1.02128886e-03,  3.12385042e-05],\n",
       "        [-9.98950945e-04,  3.36100194e-05],\n",
       "        [-9.70075615e-04,  3.70381718e-05],\n",
       "        [-9.33906812e-04,  3.96837686e-05],\n",
       "        [-8.88547870e-04,  4.24385534e-05],\n",
       "        [-8.39162226e-04,  4.43971378e-05],\n",
       "        [-7.78361041e-04,  4.49800057e-05],\n",
       "        [-7.03775269e-04,  4.81655575e-05],\n",
       "        [-6.19097660e-04,  4.77205016e-05],\n",
       "        [-5.21409180e-04,  4.31757975e-05],\n",
       "        [-4.08951886e-04,  3.43364669e-05],\n",
       "        [-2.78557345e-04,  2.28476299e-05],\n",
       "        [-1.31319139e-04,  7.96598049e-06],\n",
       "        [ 2.76523818e-05, -7.81689275e-06],\n",
       "        [ 1.98728294e-04, -2.54697444e-05],\n",
       "        [ 2.75573520e-04, -3.29682206e-05],\n",
       "        [ 3.09772693e-04, -3.76255571e-05],\n",
       "        [ 3.24496437e-04, -3.90766800e-05],\n",
       "        [ 3.31001446e-04, -4.01156024e-05],\n",
       "        [ 3.33559938e-04, -4.07744093e-05],\n",
       "        [ 3.34500193e-04, -4.11243664e-05],\n",
       "        [ 3.34831147e-04, -4.12534886e-05],\n",
       "        [ 3.34959556e-04, -4.12902013e-05],\n",
       "        [ 3.34992119e-04, -4.13162748e-05],\n",
       "        [ 3.34985395e-04, -4.13107383e-05],\n",
       "        [ 3.34971525e-04, -4.12982442e-05],\n",
       "        [ 3.34944448e-04, -4.12961782e-05],\n",
       "        [ 3.34900933e-04, -4.13077178e-05],\n",
       "        [ 3.34834243e-04, -4.13358413e-05],\n",
       "        [ 3.34761564e-04, -4.13638091e-05],\n",
       "        [ 3.34680791e-04, -4.14079713e-05],\n",
       "        [ 3.34574436e-04, -4.14843234e-05],\n",
       "        [ 3.34454916e-04, -4.15890549e-05],\n",
       "        [ 3.34326478e-04, -4.17086076e-05],\n",
       "        [ 3.34188505e-04, -4.18441178e-05],\n",
       "        [ 3.33956196e-04, -4.19621015e-05],\n",
       "        [ 3.33610829e-04, -4.21508547e-05],\n",
       "        [ 3.33211473e-04, -4.24256446e-05],\n",
       "        [ 3.32612641e-04, -4.27691471e-05],\n",
       "        [ 3.31967759e-04, -4.31580607e-05],\n",
       "        [ 3.31252338e-04, -4.35319501e-05],\n",
       "        [ 3.30219190e-04, -4.40613877e-05],\n",
       "        [ 3.28757779e-04, -4.47228868e-05],\n",
       "        [ 3.26797244e-04, -4.56029987e-05],\n",
       "        [ 3.24660096e-04, -4.67083083e-05],\n",
       "        [ 3.22058579e-04, -4.82599201e-05],\n",
       "        [ 3.18892496e-04, -4.96402396e-05],\n",
       "        [ 3.14806155e-04, -5.15541756e-05],\n",
       "        [ 3.09503447e-04, -5.40239545e-05],\n",
       "        [ 3.03778703e-04, -5.67051136e-05],\n",
       "        [ 2.97305065e-04, -6.02853781e-05],\n",
       "        [ 2.89018992e-04, -6.36323291e-05],\n",
       "        [ 2.78937606e-04, -6.64652060e-05],\n",
       "        [ 2.67392929e-04, -7.06745509e-05],\n",
       "        [ 2.52638444e-04, -7.60110091e-05],\n",
       "        [ 2.35474652e-04, -8.05458626e-05],\n",
       "        [ 2.14785739e-04, -8.69677015e-05],\n",
       "        [ 1.89700676e-04, -9.27759045e-05],\n",
       "        [ 1.61237319e-04, -9.73951210e-05],\n",
       "        [ 1.27642893e-04, -1.01092973e-04],\n",
       "        [ 9.12905300e-05, -1.05343498e-04],\n",
       "        [ 5.15475630e-05, -1.08338469e-04],\n",
       "        [ 7.11759347e-06, -1.10911904e-04],\n",
       "        [-1.30683325e-05, -1.11950869e-04],\n",
       "        [-2.15587253e-05, -1.12160022e-04],\n",
       "        [-2.54560098e-05, -1.12072052e-04],\n",
       "        [-2.71112863e-05, -1.12054905e-04],\n",
       "        [-2.78220911e-05, -1.11950660e-04],\n",
       "        [-2.81490149e-05, -1.11871983e-04],\n",
       "        [-2.82873299e-05, -1.11812285e-04],\n",
       "        [-2.83411343e-05, -1.11774391e-04],\n",
       "        [-2.83624020e-05, -1.11758168e-04],\n",
       "        [-2.83606981e-05, -1.11755215e-04],\n",
       "        [-2.83553788e-05, -1.11750593e-04],\n",
       "        [-2.83470242e-05, -1.11741406e-04],\n",
       "        [-2.83387725e-05, -1.11728081e-04],\n",
       "        [-2.83307317e-05, -1.11709485e-04],\n",
       "        [-2.83227035e-05, -1.11688664e-04],\n",
       "        [-2.83056154e-05, -1.11663869e-04],\n",
       "        [-2.82781918e-05, -1.11636402e-04],\n",
       "        [-2.82476203e-05, -1.11607461e-04],\n",
       "        [-2.82159121e-05, -1.11564284e-04],\n",
       "        [-2.81661712e-05, -1.11502832e-04],\n",
       "        [-2.81025856e-05, -1.11414014e-04],\n",
       "        [-2.80137500e-05, -1.11318444e-04],\n",
       "        [-2.79237844e-05, -1.11204252e-04],\n",
       "        [-2.78092018e-05, -1.11086381e-04],\n",
       "        [-2.76414975e-05, -1.10935444e-04],\n",
       "        [-2.74084570e-05, -1.10756514e-04],\n",
       "        [-2.71638384e-05, -1.10514695e-04],\n",
       "        [-2.68623761e-05, -1.10264336e-04],\n",
       "        [-2.64467224e-05, -1.09955541e-04],\n",
       "        [-2.58937005e-05, -1.09553155e-04],\n",
       "        [-2.53116591e-05, -1.09020414e-04],\n",
       "        [-2.45632862e-05, -1.08444971e-04],\n",
       "        [-2.36099326e-05, -1.07819948e-04],\n",
       "        [-2.23907561e-05, -1.07118315e-04],\n",
       "        [-2.08262188e-05, -1.06236498e-04],\n",
       "        [-1.91889633e-05, -1.05079607e-04],\n",
       "        [-1.74529898e-05, -1.03798244e-04],\n",
       "        [-1.56491737e-05, -1.02325927e-04],\n",
       "        [-1.37894223e-05, -1.00397479e-04],\n",
       "        [-1.12832613e-05, -9.79485390e-05],\n",
       "        [-8.03829340e-06, -9.48293228e-05],\n",
       "        [-3.92872011e-06, -9.12735960e-05],\n",
       "        [ 1.19023303e-06, -8.70266180e-05],\n",
       "        [ 6.68322661e-06, -8.15813962e-05],\n",
       "        [ 1.28334645e-05, -7.57264138e-05],\n",
       "        [ 1.99161132e-05, -6.92721489e-05],\n",
       "        [ 2.71287675e-05, -6.21273785e-05],\n",
       "        [ 3.44434890e-05, -5.33829231e-05],\n",
       "        [ 4.12018564e-05, -4.31332545e-05],\n",
       "        [ 4.34509792e-05, -3.83430163e-05],\n",
       "        [ 4.42944497e-05, -3.59194849e-05],\n",
       "        [ 4.45395405e-05, -3.46959290e-05],\n",
       "        [ 4.45411215e-05, -3.41597384e-05],\n",
       "        [ 4.44254311e-05, -3.35208655e-05],\n",
       "        [ 4.42609526e-05, -3.28893050e-05],\n",
       "        [ 4.39000279e-05, -3.22440545e-05],\n",
       "        [ 4.33885974e-05, -3.16487615e-05],\n",
       "        [ 4.28366621e-05, -3.10026620e-05],\n",
       "        [ 4.22249744e-05, -3.03559456e-05],\n",
       "        [ 4.15625525e-05, -2.95941390e-05],\n",
       "        [ 4.08203563e-05, -2.86292503e-05],\n",
       "        [ 4.00202036e-05, -2.75721412e-05],\n",
       "        [ 3.89491033e-05, -2.66977060e-05],\n",
       "        [ 3.77710890e-05, -2.55790076e-05],\n",
       "        [ 3.63601852e-05, -2.46484275e-05],\n",
       "        [ 3.47626514e-05, -2.33190202e-05],\n",
       "        [ 3.19890379e-05, -2.21891716e-05],\n",
       "        [ 2.87746692e-05, -2.13827357e-05],\n",
       "        [ 2.50725689e-05, -1.98876525e-05],\n",
       "        [ 1.96696057e-05, -1.82888317e-05],\n",
       "        [ 1.37901015e-05, -1.60915556e-05],\n",
       "        [ 7.48772401e-06, -1.35554222e-05],\n",
       "        [ 7.95112211e-07, -1.06559569e-05],\n",
       "        [-6.35622640e-06, -7.54406830e-06],\n",
       "        [-9.59313810e-06, -5.78697610e-06],\n",
       "        [-1.09014932e-05, -5.11626002e-06],\n",
       "        [-1.12865166e-05, -4.84291049e-06],\n",
       "        [-1.14507585e-05, -4.73493972e-06],\n",
       "        [-1.14658319e-05, -4.66721368e-06],\n",
       "        [-1.14716859e-05, -4.55146586e-06],\n",
       "        [-1.14402921e-05, -4.45401336e-06],\n",
       "        [-1.13796167e-05, -4.28068437e-06],\n",
       "        [-1.11925363e-05, -4.07412722e-06],\n",
       "        [-1.08593223e-05, -3.78339074e-06],\n",
       "        [-1.04904344e-05, -3.48650891e-06],\n",
       "        [-1.00121037e-05, -3.22856092e-06],\n",
       "        [-9.33351217e-06, -2.81196067e-06],\n",
       "        [-8.51191952e-06, -2.19619802e-06],\n",
       "        [-7.48846166e-06, -1.62069552e-06],\n",
       "        [-6.14193441e-06, -8.64083797e-07],\n",
       "        [-4.67166604e-06,  6.66835538e-08],\n",
       "        [-2.90444796e-06,  1.00388307e-06],\n",
       "        [-9.81736841e-07,  2.06759961e-06]])}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SGDM(three_hump_camel, derive_three_hump_camel, np.array([-1.5, -1.5]), 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1481d2fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "func:'steepest_descent' took: 0.0013 sec\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'minimum': array([-1.74755321,  0.87377874]),\n",
       " 'evaluation': 0.2986384422457916,\n",
       " 'steps': 31,\n",
       " 'path': array([[-1.5       ,  1.5       ],\n",
       "        [-1.708125  ,  1.35      ],\n",
       "        [-1.81711465,  1.230975  ],\n",
       "        [-1.72366291,  1.13811871],\n",
       "        [-1.81648029,  1.04263383],\n",
       "        [-1.64507648,  0.98689808],\n",
       "        [-1.75463477,  0.95281643],\n",
       "        [-1.75356293,  0.93402985],\n",
       "        [-1.75148293,  0.91693557],\n",
       "        [-1.75057007,  0.90217498],\n",
       "        [-1.7487293 ,  0.89061279],\n",
       "        [-1.74937157,  0.88222911],\n",
       "        [-1.74511161,  0.87755848],\n",
       "        [-1.75746724,  0.87384145],\n",
       "        [-1.73451962,  0.87565891],\n",
       "        [-1.74891576,  0.87409876],\n",
       "        [-1.7470985 ,  0.87417881],\n",
       "        [-1.74788916,  0.87401041],\n",
       "        [-1.74726925,  0.87398928],\n",
       "        [-1.74797241,  0.87385268],\n",
       "        [-1.74748766,  0.87387839],\n",
       "        [-1.7475903 ,  0.87384729],\n",
       "        [-1.7475362 ,  0.87383283],\n",
       "        [-1.74757828,  0.87381129],\n",
       "        [-1.74752235,  0.87380244],\n",
       "        [-1.74756136,  0.8737942 ],\n",
       "        [-1.74755039,  0.87379096],\n",
       "        [-1.74755593,  0.87378643],\n",
       "        [-1.74755018,  0.87378351],\n",
       "        [-1.74755714,  0.87378002],\n",
       "        [-1.74755151,  0.87377972],\n",
       "        [-1.74755321,  0.87377874]])}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "steepest_descent(three_hump_camel, derive_three_hump_camel, np.array([-1.5, 1.5]), 0.1, 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "573f9b35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "func:'stochastic_gradient_descent' took: 0.0042 sec\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'minimum': array([-1.31612992e-06,  4.18231445e-07]),\n",
       " 'evaluation': 3.0888665563219564e-12,\n",
       " 'steps': 47,\n",
       " 'path': array([[-1.50000000e+00, -1.50000000e+00],\n",
       "        [-1.49471950e+00, -1.50104590e+00],\n",
       "        [-8.53041164e-01, -8.18713498e-01],\n",
       "        [-3.77820990e-01, -2.78418024e-02],\n",
       "        [-3.85847412e-01,  9.04796580e-02],\n",
       "        [ 9.35907594e-02,  1.23051873e-02],\n",
       "        [ 2.85125131e-02,  7.79807344e-02],\n",
       "        [-4.04316414e-02, -5.57376922e-02],\n",
       "        [ 1.02993667e-01,  6.73605948e-02],\n",
       "        [-4.09840539e-02,  9.94505511e-02],\n",
       "        [-6.05078782e-02,  5.02503045e-02],\n",
       "        [-4.84722890e-02,  7.37971522e-02],\n",
       "        [-1.74591638e-02,  9.08256664e-02],\n",
       "        [-3.08650602e-03,  8.40082170e-02],\n",
       "        [ 1.35497327e-02,  3.85768405e-02],\n",
       "        [-2.91972366e-02, -3.30115328e-04],\n",
       "        [ 2.85315452e-02, -1.09158421e-02],\n",
       "        [ 1.24649411e-02, -4.15269526e-02],\n",
       "        [ 3.50068054e-03, -3.86797070e-02],\n",
       "        [ 8.99219691e-03, -3.94344277e-02],\n",
       "        [ 2.12305333e-02, -1.18561953e-02],\n",
       "        [-5.78283800e-03,  7.93442825e-03],\n",
       "        [ 2.90492161e-03,  9.44200232e-03],\n",
       "        [-5.58107271e-03,  6.59880480e-03],\n",
       "        [-5.43267657e-03,  6.87318561e-03],\n",
       "        [ 1.66006479e-03,  2.84167082e-03],\n",
       "        [-1.80226331e-03, -2.63871341e-03],\n",
       "        [ 2.36651235e-03, -4.30911656e-03],\n",
       "        [ 8.94379041e-05, -3.05786079e-03],\n",
       "        [-2.12092351e-04, -2.87255831e-03],\n",
       "        [ 1.49402035e-04,  2.58996892e-04],\n",
       "        [-3.84136465e-04,  2.25614955e-04],\n",
       "        [ 2.66863100e-06,  2.56203282e-04],\n",
       "        [-4.96746119e-05,  2.67733366e-04],\n",
       "        [-1.14130495e-04,  7.05746901e-05],\n",
       "        [ 7.47353241e-05,  1.07522341e-04],\n",
       "        [ 7.00999336e-05,  1.13290571e-04],\n",
       "        [ 8.31641802e-05,  4.89526708e-05],\n",
       "        [-5.47275785e-05, -2.50063506e-05],\n",
       "        [ 4.62395358e-05,  3.44992368e-05],\n",
       "        [-7.58628675e-05, -1.49791883e-05],\n",
       "        [-2.07757016e-05,  4.21559212e-05],\n",
       "        [-1.24097202e-05,  4.39516442e-05],\n",
       "        [ 3.14345433e-06,  2.77586276e-05],\n",
       "        [-1.32721216e-05,  2.90306853e-05],\n",
       "        [ 2.58631926e-06,  5.97304104e-06],\n",
       "        [-3.31816376e-06, -6.09940239e-06],\n",
       "        [-1.31612992e-06,  4.18231445e-07]])}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stochastic_gradient_descent(three_hump_camel, \n",
    "                            derive_three_hump_camel, \n",
    "                            np.array([-1.5, -1.5]), 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "653bd348",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run SGDM multiple times starting at different (x,y) positions and return average steps\n",
    "def run_multiple_times_SGDM(num_runs, func, first_derivative, starting_point, stepsize):\n",
    "\n",
    "    steps = []\n",
    "\n",
    "    print(f\"Running {num_runs} times\")\n",
    "\n",
    "    for i in range(num_runs):\n",
    "        steps.append(SGDM(func, first_derivative, starting_point, stepsize)['steps'])\n",
    "\n",
    "        # in case of error, print the starting point\n",
    "        if steps[-1] < 50:\n",
    "            print(f\"SGD Overflow at {starting_point}\")\n",
    "\n",
    "    return np.mean(steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ce475972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running 5 times\n",
      "func:'SGDM' took: 0.0278 sec\n",
      "func:'SGDM' took: 0.0172 sec\n",
      "func:'SGDM' took: 0.0155 sec\n",
      "func:'SGDM' took: 0.0191 sec\n",
      "func:'SGDM' took: 0.0133 sec\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "418.6"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_multiple_times_SGDM(5, three_hump_camel, derive_three_hump_camel, np.array([-1.5, -1.5]), 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eaff1f0",
   "metadata": {},
   "source": [
    "In terms of number of steps, SGDM performs worse than the other algorithms. Although SGDM seems to find the global minimum more often than CG or BFGS, SGD is also able to find the global minimum and converges to very small values a good amount of time as well. In addition, SGD only takes around 60 steps to do so. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
